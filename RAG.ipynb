{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1NYqixKow1MDOJRKoDSIQuI_qk4TJ_GYU",
      "authorship_tag": "ABX9TyOfETG2xbQc51kaDYfMO1aO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasakash26/TLDR_bot/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tasks\n",
        "- semantic search\n",
        "- lang graph orchestration\n",
        "----\n",
        "- coversation style chat\n",
        "- chat memory\n",
        "- elastic search/ BM25\n",
        "- contextual embading\n",
        "- reanking of retreved chunks\n",
        "- rifining output via llm\n"
      ],
      "metadata": {
        "id": "5hwjHD7rqJiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzRd2CTQpBEw",
        "outputId": "c1df72cf-a0fa-484a-ad99-0472d5c534be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> All components initialized successfully!\n",
            "> Chat Model: models/gemini-2.5-flash\n",
            "> Embeddings Model: sentence-transformers/all-mpnet-base-v2\n",
            "> Vector Store: <langchain_chroma.vectorstores.Chroma object at 0x791d8dc7c770>\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet \"langchain[google-genai]\" langchain-huggingface langchain-chroma langchain-text-splitters langchain-community langgraph \"unstructured[pdf]\"\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"rag_collection\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"./chroma_db\",\n",
        ")\n",
        "\n",
        "print(\"> All components initialized successfully!\")\n",
        "print(\"> Chat Model:\", llm.model)\n",
        "print(\"> Embeddings Model:\", embeddings.model_name)\n",
        "print(\"> Vector Store:\", vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders.unstructured import UnstructuredFileLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "# load the doc and split into chunks\n",
        "\n",
        "# bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "# loader = WebBaseLoader(\n",
        "#     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "#     bs_kwargs={\"parse_only\": bs4_strainer},\n",
        "# )\n",
        "# docs = loader.load()\n",
        "\n",
        "path = \"/content/drive/MyDrive/notes/SIH2025_HEARME.pdf\"\n",
        "doc = UnstructuredFileLoader(path)\n",
        "print(doc.file_path)\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    add_start_index=True,\n",
        ")\n",
        "\n",
        "all_splits = doc.load_and_split(text_splitter)\n",
        "print(all_splits[0].metadata)\n",
        "print(f\"Split document post into {len(all_splits)} sub-documents.\")\n",
        "\n",
        "# store in vector db as embeddings\n",
        "document_ids = vector_store.add_documents(documents=all_splits)"
      ],
      "metadata": {
        "id": "8oxK4SesBeTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5761a8-19a3-46b0-a198-e68e1771d04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/notes/SIH2025_HEARME.pdf\n",
            "Warning: No languages specified, defaulting to English.\n",
            "{'source': '/content/drive/MyDrive/notes/SIH2025_HEARME.pdf', 'start_index': 0}\n",
            "Split document post into 7 sub-documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# part 1: simpler model\n",
        "# custom prompt\n",
        "from langchain import hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# control flow\n",
        "from langchain_core.documents import Document\n",
        "from typing_extensions import List, TypedDict\n",
        "\n",
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str\n",
        "\n",
        "def retreve(state: State):\n",
        "    docs = vector_store.similarity_search(state[\"question\"])\n",
        "    return {\"context\": docs}\n",
        "\n",
        "def generate(state: State):\n",
        "    docs_content = \"\\n\\n\".join([doc.page_content for doc in state[\"context\"]])\n",
        "    messages = prompt.invoke({\n",
        "        \"context\": docs_content,\n",
        "        \"question\": state[\"question\"]\n",
        "    }).to_messages()\n",
        "    res = llm.predict_messages(messages)\n",
        "    return {\"answer\": res.content}\n",
        "\n",
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "graph = StateGraph(State).add_sequence([retreve, generate]).add_edge(START, \"retreve\").compile()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "Qzpj4Fx3tIGd",
        "outputId": "d0a78b19-1ef4-4edb-f520-8b097be01f0e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langgraph'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3383459030.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlanggraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTART\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStateGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStateGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mretreve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"retreve\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langgraph'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# res = graph.invoke({\"question\": \"point out the contrast in the chat month wise judging from the time stamp over the last two year\"})\n",
        "res = graph.invoke({\"question\": \"explain the mvp for this project\"})\n",
        "\n",
        "# print(f\"Context: {res['context']}\\n\\n\")\n",
        "# for doc in res[\"context\"]:\n",
        "#     print(doc.page_content)\n",
        "\n",
        "fAns = res[\"answer\"].split(\". \")\n",
        "\n",
        "formatted_answer = \"\"\n",
        "for i, line in enumerate(fAns):\n",
        "    words = line.split()\n",
        "    current_line = \"  - \" # Add indentation for list items\n",
        "    for word in words:\n",
        "        if len(current_line.split()) < 10: # Check line length\n",
        "            current_line += word + \" \"\n",
        "        else:\n",
        "            formatted_answer += current_line.strip() + \"\\n\"\n",
        "            current_line = \"    \" + word + \" \" # New line with indentation\n",
        "    formatted_answer += current_line.strip() + \"\\n\"\n",
        "\n",
        "print(formatted_answer)\n",
        "\n",
        "\n",
        "output_filename = \"res.txt\"\n",
        "\n",
        "with open(output_filename, \"w\") as f:\n",
        "    f.write(formatted_answer)\n",
        "\n",
        "print(f\"Answer saved to {output_filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP0U9vSVoaW1",
        "outputId": "fa87912e-32fa-4d07-b27b-82fcfd8e4f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- The MVP for this project would include an Agentic\n",
            "AI Chatbot to provide immediate first-aid support and alert authorities\n",
            "in high-risk situations\n",
            "- It would also feature a confidential booking system for\n",
            "students to anonymously schedule appointments with on-campus professionals\n",
            "- Finally, a localized resource hub offering psychoeducational content in\n",
            "regional languages would provide essential self-help tools.\n",
            "\n",
            "Answer saved to res.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualisation\n",
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph().draw_mermaid_png())"
      ],
      "metadata": {
        "id": "bc8N40vRtReK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agentic conversational RAG\n",
        "from langgraph.graph import MessagesState, StateGraph\n",
        "from langchain_core.tools import tool\n",
        "from langgraph_core.messages import SystemMessage\n",
        "\n",
        "agent_builder = StateGraph(MessagesState)\n",
        "\n",
        "@tool(response_format=\"content_and_artifact\")\n",
        "def retrieve(query: string):\n",
        "  \"\"\"Retreve information related to query.\"\"\"\n",
        "  retrieved_docs = vector_store.similarity_search(query, k=5)\n",
        "  serialized = \"\\n\\n\".join((f\"Source: {doc.source}\\nContent: {docs.page_content}\")for doc in retreved_docs)\n",
        "  return serialized, retrieved_docs\n",
        "\n",
        "def query_or_respond(state: Messagestate):\n",
        "  \"\"\"Generate tool-call for retreve or respond directly.\"\"\"\n",
        "  llm_with_tools = llm.bind_tools([retrieve])\n",
        "  res = llm_wit_tools.invoke(state[\"messages\"])\n",
        "  return {\"messages\":[res]}\n",
        "\n",
        "tools = ToolNode([retrieve])\n",
        "\n",
        "def generate(state:MessageState):\n",
        "  \"\"\"Generate answer to the question.\"\"\"\n",
        "  # Get generated ToolMessages\n",
        "  recent_tool_messages = []\n",
        "  for message in reversed(state[\"messages\"]):\n",
        "    if message.type == \"tool\":\n",
        "      recent_tool_messages.append(message)\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  tool_messages = recent_tool_messages[::-1]\n",
        "\n",
        "  # Format into prompt\n",
        "  docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
        "  system_message_content = (\n",
        "        \"You are an assistant for question-answering tasks. \"\n",
        "        \"Use the following pieces of retrieved context to answer \"\n",
        "        \"the question. If you don't know the answer, say that you \"\n",
        "        \"don't know. Use five sentences maximum and keep the \"\n",
        "        \"answer concise and to the point.\"\n",
        "        \"\\n\\n\"\n",
        "        f\"{docs_content}\"\n",
        "    )\n",
        "\n",
        "  conversation_messages = [\n",
        "      message\n",
        "      for message in state[\"messages\"]\n",
        "      if message.type in (\"human\", \"system\")\n",
        "      or (message.type == \"ai\" and not message.tool_calls)\n",
        "  ]\n",
        "\n",
        "  prompt = [SystemMessage(system_message_content)]+conversation_messages\n",
        "\n",
        "  # Run\n",
        "  response = llm.invoke(prompt)\n",
        "  return {\"messages\": [response]}\n",
        "\n"
      ],
      "metadata": {
        "id": "xpioy9G3RydP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the agent\n",
        "from langgraph.graph import END\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "agent_builder.add_node(query_or_respond).add_node(tools).add_node(generate)\n",
        "agent_builder.set_entry_point(\"query_or_respond\")\n",
        "agent_builder.add_conditional_edges(\n",
        "    tools_condition,\n",
        "    \"query_or_respond\",\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "M3nZ2v4Z3mK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}